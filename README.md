# Proyecto Final 2025-1: AI Neural Network                                                                                                                                                                                    
CS2013 Programaci√≥n III ¬∑ Informe Final

---

## üéÆ Descripci√≥n

Implementaci√≥n de una red neuronal multicapa en C++ para clasificaci√≥n de d√≠gitos manuscritos, incluyendo una biblioteca de √°lgebra gen√©rica, un framework de redes neuronales modular, y herramientas para carga de datos MNIST y procesamiento de im√°genes PNG.

---

## üë• Datos generales

* **Tema**: Redes Neuronales en AI
* **Grupo**: Null_Terminators
* **Desarrolladores**:
    * Franco Aedo Farge 
    * F√°tima Vill√≥n Z√°rate 
    * Josue Luna Rocha 
* Este proyecto es el **Laboratorio 1.02 de Programaci√≥n III (CS2013)** del ciclo **2025-1.**

---

## üíª Requisitos e instalaci√≥n

* **Compilador**: C++20 o superior
* **Dependencias**:
    * CMake 3.10+
    * Biblioteca `lodepng` (incluida en el proyecto)
* **Instalaci√≥n**:
    ```bash
    mkdir build        # Crea un directorio para la compilaci√≥n
    cd build           # Navega al directorio 'build'
    cmake ..           # Configura el proyecto usando CMake
    make               # Compila el c√≥digo fuente
    ```
  Esto generar√° el ejecutable `digit_classifier` (o similar, dependiendo de tu `CMakeLists.txt`) dentro del directorio `build`.


### üíæ Preparaci√≥n de los Datos

1.  **Descarga el Conjunto de Datos MNIST**: Dir√≠gete a la p√°gina oficial de MNIST y descarga los siguientes archivos comprimidos:
    * `train-images-idx3-ubyte.gz`
    * `train-labels-idx1-ubyte.gz`
2.  **Descomprime y Organiza**: Aseg√∫rate de que los archivos est√©n **descomprimidos** (sin la extensi√≥n `.gz`) y col√≥calos en el directorio `data/` de tu proyecto. Los nombres exactos de los archivos deben ser:
    * `train-images-idx3-ubyte`
    * `train-labels-idx1-ubyte`
3.  **A√±ade tus Propias Im√°genes (Opcional)**: Si quieres ver c√≥mo el clasificador se desempe√±a con tus propios dibujos, coloca im√°genes de d√≠gitos en formato PNG (por ejemplo, `0m.png`, `1m.png`, etc.) dentro del directorio `images/`. ¬°El programa las redimensionar√° autom√°ticamente a 28x28 si es necesario!

---

## üß© Investigaci√≥n te√≥rica

**Objetivo**: Comprender fundamentos y arquitecturas clave de redes neuronales (NNs).

### Historia y evoluci√≥n de las redes neuronales (NNs)

Las NNs, inspiradas en el cerebro, evolucionaron desde el modelo de **McCulloch y Pitts (1943)** y la **Regla de Hebb (1949)**. El **Perceptr√≥n de Rosenblatt (1957)** fue pionero, pero las cr√≠ticas de **Minsky y Papert (1969)** causaron un "invierno de IA". El campo resurgi√≥ en los 80 con la popularizaci√≥n de la **retropropagaci√≥n (Werbos 1974, Rumelhart, Hinton, Williams 1986)**, consolidando las NNs como el n√∫cleo de la **IA moderna** en **visi√≥n por computadora**, **PNL** y **rob√≥tica** [5].

### Principales arquitecturas de redes neuronales

* **a) MLP (Perceptr√≥n Multicapa)**
  Red *feedforward* con capas ocultas, resuelve problemas **no lineales**. Aprende v√≠a **retropropagaci√≥n**. Es un **aproximador universal** de funciones continuas, pero su entrenamiento puede ser complejo [6].

* **b) CNN (Redes Neuronales Convolucionales)**
  Especializadas en **datos espaciales (im√°genes)**. Usan **filtros convolucionales** para extraer caracter√≠sticas jer√°rquicas, capas de *pooling* y densas. Reducen par√°metros, ideales para **visi√≥n por computadora** [3].

* **c) RNN (Redes Neuronales Recurrentes)**
  Dise√±adas para **datos secuenciales**. Tienen **conexiones c√≠clicas** y **"memoria"** interna. Comparten par√°metros, manejan secuencias de longitud variable, cruciales en **PNL** y **reconocimiento de voz** [2].

### Algoritmos de entrenamiento

* **a) Retropropagaci√≥n**
  Algoritmo fundamental para entrenar NNs multicapa. Minimiza el **error** ajustando **pesos** usando **gradiente descendente**. Calcula derivadas parciales del error, propagando la informaci√≥n **hacia atr√°s** desde la salida, requiriendo **funciones de activaci√≥n diferenciables** y una **tasa de aprendizaje** adecuada [4].

* **b) Optimizadores**
  Algoritmos para **ajustar par√°metros** y minimizar la **funci√≥n de p√©rdida**. Incluyen:
    * **Descenso de Gradiente**: Cl√°sico, computacionalmente costoso.
    * **SGD (Descenso de Gradiente Estoc√°stico)**: M√°s eficiente con lotes de datos.
    * **Momento**: Acelera la convergencia de SGD.
    * **Optimizadores adaptativos (Adagrad, RMSprop)**: Ajustan din√°micamente la **tasa de aprendizaje** por par√°metro.
      La elecci√≥n depende de los datos y el modelo [1].

---

## üôåüèª Dise√±o e implementaci√≥n

### Arquitectura de la soluci√≥n

El proyecto sigue una arquitectura modular orientada a objetos en C++, aunque no se mencionan expl√≠citamente patrones de dise√±o espec√≠ficos como Factory o Strategy, la estructura de clases como `Layer`, `Dense`, `Activation`, `NeuralNetwork` y `DigitClassifier` refleja un dise√±o claro y extensible.

---
## üìÇ Estructura del Proyecto

La organizaci√≥n del c√≥digo est√° pensada para la modularidad y la claridad:

```
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îú‚îÄ‚îÄ utec/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DigitClassifier.h      // Define la interfaz del clasificador de d√≠gitos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ algebra/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Tensor.h               // Implementaci√≥n de la clase Tensor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nn/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ activation.h           // Funciones de activaci√≥n (ReLU, Softmax)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dense.h                // Capa densa (fully connected)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ layer.h                // Interfaz base para todas las capas
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ loss.h                 // Funciones de p√©rdida (Cross-Entropy)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ neural_network.h       // Clase principal de la red neuronal
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ mnist_loader.h             // Utilidades para cargar datos MNIST
‚îÇ       ‚îî‚îÄ‚îÄ lodepng.h                  // Biblioteca para cargar PNG (tercero)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ utec/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DigitClassifier.cpp    // Implementaci√≥n del clasificador de d√≠gitos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.cpp                   // Punto de entrada principal para entrenamiento y predicci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ mnist_loader.cpp           // Implementaci√≥n de la carga de datos MNIST
‚îÇ       ‚îî‚îÄ‚îÄ lodepng.cpp                // Implementaci√≥n de la biblioteca lodepng
‚îú‚îÄ‚îÄ data/                               // Directorio para los archivos de datos MNIST (ej: train-images-idx3-ubyte)
‚îú‚îÄ‚îÄ images/                             // Directorio para im√°genes PNG de prueba (ej: 0m.png, 1m.png, etc.)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ CMakeLists.txt                      // Archivo de configuraci√≥n de CMake
‚îî‚îÄ‚îÄ README.md                           // Este archivo
```                         
---



## üèÜCumplimiento de los epics

### Epic 1: Biblioteca Gen√©rica de √Ålgebra (`utec::algebra::Tensor`)
Este Epic se centra en la construcci√≥n de una base s√≥lida para cualquier operaci√≥n num√©rica compleja, esencial para el funcionamiento interno de una red neuronal. Nuestra implementaci√≥n del `Tensor` busca replicar la versatilidad de librer√≠as como NumPy en C++.

| Requisito del Enunciado                                        | Detalles de Implementaci√≥n                                                                                                                                                                                                                                                                                                  |
| :------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Constructores vari√°dicos y `std::array<size_t, Rank>`          | La implementaci√≥n actual utiliza `std::vector<size_t>` e `initializer_list<size_t>` para la forma. El acceso multidimensional se realiza a trav√©s de `at(const std::vector<size_t>& indices)` y acceso lineal con `operator[]`. La funcionalidad es similar, aunque la sintaxis vari√°dica espec√≠fica no est√° presente. |
| Acceso vari√°dico `operator()`                                  | Sustituido por `at(const std::vector<size_t>& indices)`.                                                                                                                                                                                                                                                                 |
| `shape()`                                                      | Retorna el `std::vector<size_t>` que representa la forma.                                                                                                                                                                                                                                                                 |
| `reshape()` (preservando elementos)                            | La clase `Tensor` no tiene un m√©todo `reshape` expl√≠cito.                                                                                                                                                                                                                                                                 |
| `fill()`                                                       | Permite rellenar el tensor con un valor escalar.                                                                                                                                                                                                                                                                           |
| Operadores aritm√©ticos (`+`, `-`, `*` escalar, `*` tensor)    | `+` y `-` implementados. `*` escalar implementado. El producto matricial se maneja con `matmul()`. El "broadcasting impl√≠cito" no est√° implementado de forma gen√©rica en `operator*`, pero la multiplicaci√≥n matricial lo maneja en el contexto de la red neuronal.                                                      |
| `transpose_2d()` (para Rank=2)                                 | Implementada una funci√≥n `transpose` separada dentro de la clase `Dense` que utiliza para transponer tensores 2D.                                                                                                                                                                                                        |

---
### Epic 2: Red Neuronal Full (`utec::nn`)
Este Epic se enfoca en la construcci√≥n del core de la inteligencia artificial: el framework de redes neuronales, incluyendo sus componentes esenciales como capas y funciones de activaci√≥n/p√©rdida.

| Requisito del Enunciado                                        | Detalles de Implementaci√≥n                                                                                                                                                                                                                                                                                                    |
| :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `ILayer` (ahora `Layer`) con `forward` y `backward`           | La clase `Layer` define la interfaz con los m√©todos `forward` y `backward` (que recibe el `learning_rate` para la actualizaci√≥n de pesos dentro de la capa).                                                                                                                                                            |
| `Dense` con `W`, `b`, `dW`, `db` y `last_x` (ahora `input_`)  | La clase `Dense` gestiona sus pesos (`weights_`), sesgos (`bias_`) y la entrada previa (`input_`) para el *backpropagation*. Las actualizaciones de `dW` y `db` se aplican directamente a `weights_` y `bias_` dentro del `backward` de la capa.                                                         |
| `ReLU` con `mask`                                              | La clase `Activation` implementa `ReLU` y maneja su derivada eficientemente. No usa una `mask` expl√≠cita, sino que compara `input_.data()[i] > 0` en el `backward`.                                                                                                                                          |
| `MSELoss` con `forward` y `backward`                           | Se implement√≥ `cross_entropy` y `cross_entropy_derivative` en el namespace `utec::nn::loss` en lugar de `MSELoss`, ya que la entrop√≠a cruzada es m√°s adecuada para problemas de clasificaci√≥n multiclase como MNIST.                                                                                                |
| Clase `NeuralNetwork` (`add_layer`, `forward`, `backward`, `optimize`, `train`) | La clase `NeuralNetwork` permite construir la red con `add_layer`. `forward` propaga la entrada. `train_step` encapsula el `backward` y la actualizaci√≥n de pesos (que se delega a las capas individuales). El m√©todo `train` de `DigitClassifier` orquesta el entrenamiento sobre m√∫ltiples √©pocas e im√°genes. |

---
### Epic 3: Agente (`utec::agent::DigitClassifier`)
Originalmente concebido para un agente de Pong, este Epic se adapta para la creaci√≥n de un clasificador de d√≠gitos, demostrando c√≥mo la red neuronal puede ser encapsulada y utilizada para una tarea espec√≠fica de percepci√≥n.

| Requisito del Enunciado                                        | Detalles de Implementaci√≥n                                                                                                                                                                                                                                                                  |
| :------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Clase `PongAgent` (adaptado a `DigitClassifier`)              | La clase `DigitClassifier` cumple el rol de `PongAgent`, pero para el dominio de clasificaci√≥n de d√≠gitos.                                                                                                                                                                                       |
| Recibir `State` (adaptado a `Tensor` de imagen)               | El m√©todo `predict` del `DigitClassifier` recibe un `utec::algebra::Tensor` que representa la imagen (equivalente al `State` del Pong).                                                                                                                                                                |
| Decidir acci√≥n (`act()`, adaptado a `predict()`)              | El m√©todo `predict` de `DigitClassifier` devuelve el √≠ndice de la clase predicha (el d√≠gito), que es an√°logo a la "acci√≥n" en el contexto del Pong.                                                                                                                                                             |
| Bucle de simulaci√≥n con `forward`                             | El `main.cpp` demuestra este bucle tanto para las im√°genes de prueba autom√°ticas como para el modo interactivo, donde `forward` (v√≠a `predict`) es invocado para cada nueva entrada.                                                                                                                              |

---
### Epic 4: Paralelismo y CUDA Opcional
Este Epic explora la optimizaci√≥n del rendimiento de la red neuronal a trav√©s de la computaci√≥n paralela, incluyendo el uso de hilos o la aceleraci√≥n por GPU (CUDA).

| Requisito del Enunciado                                        | Detalles de Implementaci√≥n                                                                                                                                                                                                                                                                  |
| :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `ThreadPool` y cola concurrente para inferencias             | Esta funcionalidad no se ha incluido en la implementaci√≥n actual.                                                                                                                                                                                                                           |
| Soporte CUDA                                                   | La implementaci√≥n es puramente en CPU.                                                                                                                                                                                                                                                      |

---
### Epic 5: Entrenamiento, Validaci√≥n y Documentaci√≥n
Este Epic abarca las fases cruciales del ciclo de vida de un modelo de Machine Learning: c√≥mo se entrena, c√≥mo se eval√∫a su rendimiento y c√≥mo se documenta para su comprensi√≥n y uso.

| Requisito del Enunciado                                        | Detalles de Implementaci√≥n                                                                                                                                                                                                                                                                  |
| :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Pipeline de entrenamiento b√°sico                               | El `main.cpp` y la clase `DigitClassifier` implementan un ciclo de entrenamiento b√°sico con los datos de MNIST.                                                                                                                                                                       |
| Serializaci√≥n del modelo                                       | La capacidad de guardar y cargar el modelo entrenado no est√° implementada.                                                                                                                                                                                                                   |
| Validaci√≥n (Conjunto separado)                                 | El c√≥digo actual solo usa datos de entrenamiento; no hay un conjunto de validaci√≥n separado.                                                                                                                                                                                                 |
| Documentaci√≥n                                                  | Este `README.md` cumple con el objetivo de documentaci√≥n, explicando la estructura, el uso y el cumplimiento de los requisitos. 

---

## üöÄ Manual de Uso y Casos de Prueba

Despu√©s de la compilaci√≥n, el ejecutable `digit_classifier` estar√° disponible en el directorio `build`. Este programa permite entrenar la red neuronal con el conjunto de datos MNIST y realizar predicciones.

Para ejecutar el entrenamiento y la predicci√≥n, usar√°s:
```bash
./build/digit_classifier
```

El programa carga autom√°ticamente los datos de MNIST, entrena la red (mostrando progreso y p√©rdida promedio por √©poca) y clasifica im√°genes PNG presentes en el directorio `images/`.

### Casos de prueba

* **Carga de Datos MNIST**: Verificar la correcta carga y preprocesamiento de im√°genes y etiquetas.
* **Funcionalidad de Tensor**: Validar operaciones b√°sicas (suma, resta, multiplicaci√≥n escalar, producto matricial).
* **Forward y Backward Pass**: Probar la propagaci√≥n hacia adelante y hacia atr√°s en capas `Dense` y `Activation` para asegurar el flujo de datos y c√°lculo de gradientes.
* **Convergencia del Entrenamiento**: Observar la reducci√≥n de la p√©rdida promedio por √©poca para confirmar el aprendizaje de la red.
* **Clasificaci√≥n de Im√°genes Personalizadas**: Evaluar la capacidad del clasificador para predecir d√≠gitos en im√°genes PNG (redimensionadas y normalizadas).

---

##  üîé An√°lisis del Rendimiento

### M√©tricas de Evaluaci√≥n

* **Progreso de Entrenamiento**: Se monitorea el avance por **√©pocas**, con reportes de progreso cada **1000 muestras** procesadas.
* **Tiempo de Entrenamiento**: Actualmente **no se mide expl√≠citamente**. Se considera integrar un temporizador para el registro futuro.
* **P√©rdida Promedio**: Se calcula y reporta la **p√©rdida promedio por √©poca** utilizando la funci√≥n de **entrop√≠a cruzada**.
* **Precisi√≥n de Clasificaci√≥n**: La **precisi√≥n final no se calcula** sobre un conjunto de validaci√≥n, lo que representa una potencial mejora para una evaluaci√≥n m√°s robusta.

### Fortalezas y Limitaciones

* **Fortalezas**:
    * **Implementaci√≥n Pura C++**: C√≥digo desarrollado desde cero con **dependencias m√≠nimas**, facilitando la comprensi√≥n interna.
    * **Arquitectura Modular**: Dise√±o claro y extensible para futuras adiciones.
* **Limitaciones**:
    * **Ejecuci√≥n Exclusiva en CPU**: Rendimiento **limitado por CPU**, restringiendo la escalabilidad a grandes datasets.
    * **Ausencia de Paralelizaci√≥n**: No se ha implementado **paralelismo (Epic 4)**, impidiendo una aceleraci√≥n significativa.
    * **Falta de Conjunto de Validaci√≥n**: Impide una evaluaci√≥n objetiva de la **generalizaci√≥n** y detecci√≥n de **sobreajuste (overfitting)**.


### Mejoras Futuras

* **Optimizaci√≥n de √Ålgebra Lineal**: Integrar bibliotecas como **BLAS** para acelerar **operaciones tensoriales** (ej. producto matricial).
* **Paralelizaci√≥n del Entrenamiento**: Implementar **paralelismo por lotes** (e.g., ThreadPools) o soporte **CUDA** para **aceleraci√≥n por GPU**.
* **Persistencia del Modelo**: A√±adir funcionalidad para **guardar y cargar modelos entrenados (serializaci√≥n)**.
* **Evaluaci√≥n Robusta**: Incorporar un **conjunto de validaci√≥n dedicado** para medir la **generalizaci√≥n** y monitorizar el **overfitting**.

---

## üíØ Trabajo en equipo

| Tarea                      | Miembro            | Rol                               |
| :------------------------- | :----------------- |:----------------------------------|
| Investigaci√≥n te√≥rica      | Franco Aedo Farge  | Contribuci√≥n general al proyecto. |
| Dise√±o de la arquitectura  | F√°tima Vill√≥n Z√°rate | Contribuci√≥n general al proyecto. |
| Implementaci√≥n del modelo  | Josue Luna Rocha   | Contribuci√≥n general al proyecto. |
| Documentaci√≥n              | Equipo             | Creaci√≥n de `README.md`.          |

---

## ‚ú® Conclusiones

**Logros**:
* Se implement√≥ una biblioteca de √°lgebra gen√©rica (`utec::algebra::Tensor`) para operaciones tensoriales fundamentales.
* Se construy√≥ un framework de redes neuronales (`utec::nn`) modular desde cero, incluyendo capas (`Dense`, `Activation`) y funciones de p√©rdida (`cross_entropy`), permitiendo la creaci√≥n de arquitecturas complejas.
* El `DigitClassifier` demuestra la aplicaci√≥n de la red para la clasificaci√≥n de d√≠gitos manuscritos del conjunto de datos MNIST, con capacidad de carga de datos e inferencia.

**Aprendizajes**:
* Profundizaci√≥n en el algoritmo de *backpropagation* y la optimizaci√≥n de pesos y sesgos.
* Comprensi√≥n de la modularidad en el dise√±o de frameworks de Machine Learning.
* Manejo de estructuras de datos complejas (tensores multidimensionales) en C++.

---
## üîù Recomendaciones
Para futuras iteraciones, se recomienda explorar la implementaci√≥n de paralelismo (multi-threading o CUDA) para mejorar el rendimiento, integrar un conjunto de validaci√≥n para una evaluaci√≥n m√°s robusta del modelo, y desarrollar funcionalidades de serializaci√≥n para guardar y cargar modelos entrenados.

---

## üìö Bibliograf√≠a

[1] Analytics Vidhya. (2021, octubre 7). *Gu√≠a completa sobre optimizadores en aprendizaje profundo*. Recuperado de https://www-analyticsvidhya-com.translate.goog/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc

[2] C. Arana, *Redes neuronales recurrentes: An√°lisis de los modelos especializados en datos secuenciales* (Serie Documentos de Trabajo No. 797). Universidad del Centro de Estudios Macroecon√≥micos de Argentina (UCEMA), 2021. Recuperado de https://www.econstor.eu/handle/10419/238422

[3] Departamento de Matem√°tica Aplicada. (2021). *Redes neuronales convoluciones ‚Äì Introducci√≥n al aprendizaje autom√°tico*. Universidad Polit√©cnica de Madrid. Recuperado de https://dcain.etsin.upm.es/~carlos/bookAA/05.7_RRNN_Convoluciones_CIFAR_10_INFORMATIVO.html

[4] A. Fraga Fat√°s, *El algoritmo de retropropagaci√≥n en redes neuronales multicapa*. Trabajo de Fin de Grado, Universidad de Zaragoza, 2023. Recuperado de https://zaguan.unizar.es/record/154704/files/TAZ-TFG-2023-2967.pdf

[5] N. Laredo, *Redes Neuronales*. Tecnol√≥gico Nacional de M√©xico, Campus Laredo, s.f. Recuperado de https://nlaredo.tecnm.mx/takeyas/Apuntes/Inteligencia%20Artificial/Apuntes/tareas_alumnos/RNA/Redes%20Neuronales2.pdf

[6] Universidad de Sevilla. (s.f.). *Perceptr√≥n multicapa*. Biblus. Recuperado de https://biblus.us.es/bibing/proyectos/abreproy/12166/fichero/Volumen+1+-+Memoria+descriptiva+del+proyecto%252F3+-+Perceptron+multicapa.pdf








